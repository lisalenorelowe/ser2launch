
                            < M A T L A B (R) >
                  Copyright 1984-2023 The MathWorks, Inc.
                  R2023a (9.14.0.2206163) 64-bit (glnxa64)
                             February 22, 2023

Warning: X does not support locale en_US.UTF-8
 
To get started, type doc.
For product information, visit www.mathworks.com.
 

numCores =

    32

Starting parallel pool (parpool) using the 'Processes' profile ...
Connected to parallel pool with 32 workers.

ans = 

 ProcessPool with properties: 

            Connected: true
           NumWorkers: 32
                 Busy: false
              Cluster: Processes (Local Cluster)
        AttachedFiles: {}
    AutoAddClientPath: true
            FileStore: [1x1 parallel.FileStore]
           ValueStore: [1x1 parallel.ValueStore]
          IdleTimeout: 30 minutes (30 minutes remaining)
          SpmdEnabled: true

Hello from 26.000000
Hello from 2.000000
Hello from 8.000000
Hello from 12.000000
Hello from 25.000000
Hello from 3.000000
Hello from 4.000000
Hello from 20.000000
Hello from 17.000000
Hello from 10.000000
Hello from 21.000000
Hello from 30.000000
Hello from 5.000000
Hello from 27.000000
Hello from 28.000000
Hello from 31.000000
Hello from 19.000000
Hello from 24.000000
Hello from 6.000000
Hello from 23.000000
Hello from 15.000000
Hello from 1.000000
Hello from 32.000000
Hello from 13.000000
Hello from 16.000000
Hello from 29.000000
Hello from 14.000000
Hello from 63.000000
Hello from 18.000000
Hello from 7.000000
Hello from 11.000000
Hello from 22.000000
Hello from 9.000000
Hello from 34.000000
Hello from 40.000000
Hello from 44.000000
Hello from 58.000000
Hello from 56.000000
Hello from 35.000000
Hello from 36.000000
Hello from 52.000000
Hello from 49.000000
Hello from 41.000000
Hello from 53.000000
Hello from 61.000000
Hello from 62.000000
Hello from 37.000000
Hello from 59.000000
Hello from 60.000000
Hello from 51.000000
Hello from 55.000000
Hello from 38.000000
Hello from 42.000000
Hello from 54.000000
Hello from 47.000000
Hello from 33.000000
Hello from 64.000000
Hello from 45.000000
Hello from 48.000000
Hello from 46.000000
Hello from 68.000000
Hello from 50.000000
Hello from 39.000000
Hello from 43.000000
Hello from 57.000000
Hello from 79.000000
Hello from 83.000000
Hello from 81.000000
Hello from 65.000000
Hello from 72.000000
Hello from 82.000000
Hello from 67.000000
Hello from 78.000000
Hello from 71.000000
Hello from 69.000000
Hello from 70.000000
Hello from 74.000000
Hello from 66.000000
Hello from 80.000000
Hello from 73.000000
Hello from 75.000000
Hello from 84.000000
Hello from 77.000000
Hello from 76.000000
Hello from 91.000000
Hello from 104.000000
Hello from 93.000000
Hello from 94.000000
Hello from 92.000000
Hello from 87.000000
Hello from 89.000000
Hello from 88.000000
Hello from 95.000000
Hello from 85.000000
Hello from 90.000000
Hello from 96.000000
Hello from 86.000000
Hello from 108.000000
Hello from 112.000000
Hello from 114.000000
Hello from 99.000000
Hello from 115.000000
Hello from 100.000000
Hello from 113.000000
Hello from 101.000000
Hello from 103.000000
Hello from 102.000000
Hello from 106.000000
Hello from 98.000000
Hello from 109.000000
Hello from 105.000000
Hello from 97.000000
Hello from 107.000000
Hello from 110.000000
Hello from 111.000000
Hello from 119.000000
Hello from 120.000000
Hello from 117.000000
Hello from 116.000000
Hello from 118.000000
Parallel pool using the 'Processes' profile is shutting down.

------------------------------------------------------------
Sender: LSF System <lsfadmin@c044n02>
Subject: Job 997370: <par_matlab> in cluster <Hazel> Done

Job <par_matlab> was submitted from host <login01> by user <lllowe> in cluster <Hazel> at Sun Jun  4 14:41:37 2023
Job was executed on host(s) <6*c044n02>, in queue <debug>, as user <lllowe> in cluster <Hazel> at Sun Jun  4 14:41:37 2023
</home/lllowe> was used as the home directory.
</rsstu/users/l/lllowe/cgem/ser2launch/par_matlab> was used as the working directory.
Started at Sun Jun  4 14:41:37 2023
Terminated at Sun Jun  4 14:44:02 2023
Results reported at Sun Jun  4 14:44:02 2023

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/tcsh
#BSUB -n 6                      # Number of MPI tasks
#BSUB -R span[hosts=1]          # MPI tasks per node
##BSUB -x                        # Exclusive use of nodes
#BSUB -J par_matlab             # Name of job
#BSUB -W 0:10                   # Wall clock time
#BSUB -o par_matlab.out.%J      # Standard out
#BSUB -e par_matlab.err.%J      # Standard error

#This matlab script will autodetect and use all cores on the node

##To submit, do 
## bsub < submit_henry2.csh
module load matlab         # Set environment

#Replace the start and end for your loop indicies, i.e.:
# runscript(istart,iend)
matlab -nodisplay -nosplash -r 'runscript(1,120);exit;' 

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   742.39 sec.
    Max Memory :                                 36 GB
    Average Memory :                             25.00 GB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              36
    Max Threads :                                3436
    Run time :                                   156 sec.
    Turnaround time :                            145 sec.

The output (if any) is above this job summary.



PS:

Read file <par_matlab.err.997370> for stderr output of this job.

